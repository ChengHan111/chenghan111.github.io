<!doctype html>
<html>

<head>
  <title>Cheng Han</title>
  <meta charset="utf-8" name="viewport" content="width=device-width, initial-scale=1">
  <link href="css/frame.css" media="screen" rel="stylesheet" type="text/css" />
  <link href="css/controls.css" media="screen" rel="stylesheet" type="text/css" />
  <link href="css/custom.css" media="screen" rel="stylesheet" type="text/css" />
  <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
  <link href='https://fonts.googleapis.com/css?family=Open+Sans+Condensed:300,700' rel='stylesheet' type='text/css'>
  <link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,700" rel="stylesheet">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script src="js/menu.js"></script>
  <script>
    (function (i, s, o, g, r, a, m) {
      i['GoogleAnalyticsObject'] = r;
      i[r] = i[r] || function () {
        (i[r].q = i[r].q || []).push(arguments)
      }, i[r].l = 1 * new Date();
      a = s.createElement(o),
        m = s.getElementsByTagName(o)[0];
      a.async = 1;
      a.src = g;
      m.parentNode.insertBefore(a, m)
    })(window, document, 'script', 'https://www.google-analytics.com/analytics.js', 'ga');
    ga('create', 'UA-103598896-1', 'auto');
    ga('send', 'pageview');
  </script>
</head>

<body>
  <div class="menu-container"></div>
  <div class="content-container">
    <div class="content">
      <div class="content-table flex-column">
        <div class="flex-row">
          <div class="flex-item flex-column">
            <img class="image" id="me" src="img/han2.jpg">
          </div>
          <div class="flex-item flex-column">
            <h2>Cheng Han</h2>
            <p class="text">
              Assistant Professor <br>
              chk9k (at) umsystem.edu<br>
              <a href="https://www.umkc.edu/" target="_blank">University of Missouri -- Kansas City, USA</a><br>
            </p>
          </div>
        </div>
        <div class="flex-row">
          <div class="flex-item flex-column">
            <h2>Biography</h2>
            <hr>
            <p class="text">
              I am a Tenure-track Assistant Professor at the School of Science and Engineering, University of Missouri -- Kansas City, USA.
              My research interests include adaptable and sustainable intelligence, where I design, implement, deploy, and evaluate AI systems that empower the communities. <mark class="red">I am continuously interested in recuriting Ph.D. students with strong background in AI agents and systemic efficiency. </mark></p></b>
              
              News (08/15/2025): Our lab website is online! While it is still under construction, please feel free to check! </p>

              News (06/29/2025): Website updated for new paper published in ICCV. Congrats to Taowen!</p>

              News (03/14/2025): Website updated for new papers published in EMNLP and ICLR. Congrats to Taowen and Yiyang!
            </p>
            <h3>Table of Content</h3>
            <ul>
              <li><a href="#Selected Publications">Selected Publications</a></li>
              <li><a href="#Services">Services</a></li>
              <li><a href="#Talks and Presentation">Talks and Presentation</a></li>
              <li><a href="#Teaching">Teaching</a></li>
            </ul>
            <h2 id="Selected Publications">Selected Publications</h2>
            <hr>
            <!--This list is reversed on the website due to reverse number listing-->
            <ol class="publication A-list">
              <li>
                <p class="text-small-margin">
                  Visual Recognition with Deep Nearest Centriods <a href="https://arxiv.org/abs/2209.07383" target="_blank">[pdf(arxiv)]</a><a href="https://github.com/ChengHan111/DNC" target="_blank">[code]</a>.
                  W. Wang*†, C. Han*, T. Zhou*, D. Liu†, International Conference on Learning Representations (ICLR), 2023 (Spotlight). *Equal Contribution
                </p>
              </li>
              <li>
                <p class="text-small-margin">
                  Prompt Learns Prompt: Exploring Knowledge-Aware Generative Prompt Collaboration For Video Captioning <a href="https://www.ijcai.org/proceedings/2023/0180.pdf" target="_blank">[pdf(IJCAI)]</a>.
                  L. Yan*, C. Han*, Z. Xu, D. Liu†, Q. Wang, International Joint Conferences on Artificial Intelligence (IJCAI), 2023. *Equal Contribution
                </p>
              </li>
              <li>
                <p class="text-small-margin">
                  E^2VPT: An Effective and Efficient Approach for Visual Prompt Tuning <a href="https://arxiv.org/abs/2307.13770" target="_blank">[pdf(arxiv)]</a><a href="https://github.com/ChengHan111/E2VPT" target="_blank">[code]</a>.
                  C. Han*, Q. Wang, Y. Cui, Z. Cao, W. Wang, S. Qi, D. Liu†. International Conference on Computer Vision (ICCV), 2023.
                </p>
              </li>
              <li>
                <p class="text-small-margin">
                  Unified 3D Segmenter As Prototypical Classifiers <a href="https://openreview.net/pdf?id=Q6zd1hr7sD" target="_blank">[pdf(Openreview)]</a><a href="https://github.com/zyqin19/PROTOSEG" target="_blank">[code]</a>.
                  Z. Qin*, C. Han*, X. Lu, Q. Wang, X, Nie, Y, Yin†. Conference on Neural Information Processing Systems (NeurIPS), 2023. *Equal Contribution
                </p>
              </li>
              <li>
                <p class="text-small-margin">
                  CML-MOTS: Collaborative Multi-task Learning for Multi-Object Tracking and Segmentation <a href="https://arxiv.org/pdf/2311.00987" target="_blank">[pdf(arxiv)]</a>.
                  Y. Cui, C. Han, D. Liu†. ACM Journal on Autonomous Transportation Systems (JATS), 2023.
                </p>
              </li>
              <li>
                <p class="text-small-margin">
                  Facing the Elephant in the Room: Visual Prompt Tuning or Full Finetuning?
                  C. Han*, Q. Wang, Y. Cui, W. Wang, L. Huang, S. Qi, D. Liu†. International Conference on Learning Representations (ICLR), 2024 <a href="https://openreview.net/pdf?id=bJx4iOIOxn" target="_blank">[pdf(Openreview)]</a><a href="https://github.com/ChengHan111/VPT-or-FT/" target="_blank">[code]</a>.
                </p>
              </li>
              <li>
                <p class="text-small-margin">
                  Image Translation as Diffusion Visual Programmers.
                  C. Han, J. C. Liang, Q. Wang, M. Rabbani, S. Dianat, R. Rao, Y. N. Wu, D. Liu†. International Conference on Learning Representations (ICLR), 2024. <a href="https://openreview.net/pdf?id=yozwqhIHXj" target="_blank">[pdf(Openreview)]</a><a href="https://dvpmain.github.io/" target="_blank">[Website]</a>.
                </p>
              </li>
              <li>
                <p class="text-small-margin">
                  ProMotion: Prototypes As Motion Learners.
                  Y. Lu, D. Liu, Q., C. Han, Y. Cui, Z. Cao, X. Zhang, Y. V. Chen, H. Fan†. IEEE / CVF Computer Vision and Pattern Recognition Conference (CVPR), 2024. <a href="https://arxiv.org/abs/2406.04999" target="_blank">[pdf]</a>
                </p>
              </li>
              <li>
                <p class="text-small-margin">
                  SSGA-Net: Stepwise Spatial Global-local Aggregation Networks for for Autonomous Driving. Y. Cui, C. Han, D. Liu†. <a href="https://www.semanticscholar.org/reader/d10764b36c5651da9078a2a3889daa8a2117cd0f" target="_blank">[pdf]</a>
                </p>
              </li>
              <li>
                <p class="text-small-margin">
                  Prototypical Transformer as Unified Motion Learners. C. Han*, Y. Lu*, G. Sun, J. C. Liang, Z. Cao, Q. Wang, Q. Guan, S. A. Dianat, R. M. Rao, T. Geng, Z. Tao†, D. Liu†. International Conference on Machine Learning (ICML), 2024. <a href="https://arxiv.org/abs/2406.01559" target="_blank">[pdf]</a>
                </p>
              </li>
              <li>
                <p class="text-small-margin">
                  Self-supervised Adversarial Training of Monocular Depth Estimation against Physical-World Attacks. Z. Cheng, C. Han, J. C. Liang, Q. Wang, X. Zhang, D. Liu†. IEEE Transactions on Pattern Analysis and Machine Intelligence (IEEE TPAMI), 2024. <a href="https://arxiv.org/abs/2406.05857" target="_blank">[pdf]</a>
                </p>
              </li>
	      <li>
                <p class="text-small-margin">
                  Optical Flow as Spatial-Temporal Attention Learners. Y. Lu*, C. Han*, Q. Wang, H. Fan, Z. Kong, D. Liu, Y. Chen. IEEE Transactions on Pattern Analysis and Machine Intelligence (IEEE TPAMI), 2024.
                </p>
              </li>
              <li>
                <p class="text-small-margin">
                  AMD: Automatic Multi-step Distillation of Large-scale Vision Models. C. Han, Q. Wang, S. A Dianat, M. Rabbani, R. M Rao, Y. Fang, Q. Guan, L. Huang, D. Liu†. The European Conference on Computer Vision (ECCV), 2024. <a href="https://arxiv.org/abs/2407.04208" target="_blank">[pdf]</a>
                </p>
              </li>
	      <li>
                <p class="text-small-margin">
                  M^2PT: Multimodal Prompt Tuning for Zero-shot Instruction Learning. T. Wang, Y. Liu, J. C. Liang, J. Zhao, Y. Cui, Y. Mao, S. Nie, J. Liu, F. Feng, Z. Xu, C. Han, L. Huang, Q. Wang, D. Liu. The Conference on Empirical Methods in Natural Language Processing (EMNLP), 2024. <a href="https://aclanthology.org/2024.emnlp-main.218.pdf" target="_blank">[pdf]</a>
                </p>
              </li>
              <li>
                <p class="text-small-margin">
                  Visual Fourier Prompt Tuning. R. Zeng*, C. Han*, Q Wang, C. Wu, T. Geng, L. Rao, L. Huang, Y. N. Wu, D. Liu. The Conference on Neural Information Processing Systems (NeurIPS), 2024. <a href="https://proceedings.neurips.cc/paper_files/paper/2024/hash/0a0eba34ab2ff40ca2d2843324dcc4ab-Abstract-Conference.html" target="_blank">[pdf]</a>
                </p>
              </li>
              <li>
                <p class="text-small-margin">
                  Re-Imagining Multimodal Instruction Tuning: A Representation View. Y. Liu, J. C. Liang, R. Tang, Y. Lee, M. Rabbani, S. Dianat, R. Rao, L. Huang, D. Liu, Q. Wang, C. Han†. The International Conference on Learning Representations (ICLR), 2025. <a href="https://arxiv.org/pdf/2503.00723" target="_blank">[pdf]</a>
                </p>
              </li>
              <li>
                <p class="text-small-margin">
                  Exploring the Adversarial Vulnerabilities of Vision-Language-Action Models in Robotics. T. Wang*, C. Han*, J. C. Liang*, W. Yang, D. Liu, L. X. Zhang,  Q. Wang,  J. Luo, R. Tang. International Conference on Computer Vision (ICCV), 2025. <a href="https://arxiv.org/abs/2411.13587" target="_blank">[pdf]</a><a href="https://github.com/William-wAng618/roboticAttack" target="_blank">[code]</a>
                </p>
              </li>
            </ol>
            <h2 id="Services">Services</h2>
            <hr>
            <!--This list is reversed on the website due to reverse number listing-->
            <ol class="publication A-list">
              <li>
                <p class="text-small-margin">
                  <b>Faculty Search Committee</b> at the University of Missouri – Kansas City, 2024
                </p>
              <li>
                <li>
                  <p class="text-small-margin">
                    <b>Graduate Committee</b> at the University of Missouri – Kansas City, 2024
                  </p>
                <li>
              <li>
                <p class="text-small-margin">
                  <b>Program Committee</b>, The Association for the Advancement of Artificial Intelligence (AAAI), 2023 -- Now.
                </p>
              <li>
	      <li>
                <p class="text-small-margin">
                  <b>Program Committee</b>, SIAM International Conference on Data Mining (SDM), 2024.
                </p>
              <li>
              <li>
                <p class="text-small-margin">
                  <b>Reviewer</b>, The Annual Conference on Neural Information Processing Systems (NeurIPS), 2024.
                </p>
              <li>
	      <li>
                <p class="text-small-margin">
                  <b>Reviewer</b>, The International Conference on Learning Representations (ICLR), 2024.
                </p>
              <li>
              <li>
                <p class="text-small-margin">
                  <b>Reviewer</b>, The Conference on Computer Vision and Pattern Recognition (CVPR), 2022 -- Now.
                </p>
              <li>
              <li>
                <p class="text-small-margin">
                  <b>Reviewer</b>, IEEE Transactions on Circuits and Systems for Video Technology (IEEE TCSVT), 2022 -- Now.
                </p>
              <li>
              <li>
                <p class="text-small-margin">
                  <b>Reviewer</b>, Association for Computational Linguistics Rolling Review (ARR), 2023 -- Now.
                </p>
              </li>
              <li>
                <p class="text-small-margin">
                  <b>Reviewer</b>, The International World Wide Web Conference (WWW), 2024.
                </p>
              </li>
              <li>
                <p class="text-small-margin">
                  <b>Reviewer</b>, The Annual Conference on Neural Information Processing Systems (NeurIPS), 2024 - now
                </p>
              </li>
              <li>
                <p class="text-small-margin">
                  <b>Reviewer</b>, The International Conference on Learning Representations (ICLR), 2023 – now
                </p>
              </li>
              <li>
                <p class="text-small-margin">
                  <b>Reviewer</b>, The International Conference on Machine Learning (ICML), 2023 – now
                </p>
              </li>
              <li>
                <p class="text-small-margin">
                  <b>Reviewer</b>, International Journal of Computer Vision (IJCV), 2024
                </p>
              </li>
              <li>
                <p class="text-small-margin">
                  <b>Reviewer</b>, Transactions on Machine Learning Research (TMLR), 2024 - now
                </p>
              </li>
              <li>
                <p class="text-small-margin">
                  <b>Reviewer</b>, IEEE Transactions on Pattern Analysis and Machine Intelligence (IEEE TPAMI), 2024.
                </p>
              </li>
              <li>
                <p class="text-small-margin">
                  <b>Reviewer</b>, IEEE Transactions on Artificial Intelligence (IEEE TAI), 2025.
                </p>
              </li>
              <li>
                <p class="text-small-margin">
                  <b>Reviewer</b>, Neurocomputing, 2024.
                </p>
              </li>
              <li>
                <p class="text-small-margin">
                  <b>Reviewer</b>, The International Conference on Computer Vision (ICCV), 2025.
                </p>
              </li>
              <li>
                <p class="text-small-margin">
                  <b>Reviewer</b>, The Winter Conference on Applications of Computer Vision (WACV), 2026.
                </p>
              </li>
            </ol>
            <h2 id="Talks and Presentation">Talks and Presentation</h2>
            <hr>
            <ol class="publication A-list">
              <li>
                <p class="text-small-margin">
                  <b> ICLR 2023 Spotlight Presentation</b>, Visual Recognition with Deep Nearest Centriods <a href="https://www.youtube.com/watch?v=NlzTniUXoqo" target="_blank">[YouTube]</a>.
                </p>
              <li>
              <li>
                <p class="text-small-margin">
                  <b> ICCV 2023 Presentation</b>, E^2VPT: An Effective and Efficient Approach for Visual Prompt Tuning <a href="https://www.youtube.com/watch?v=T6qOdypJ5Kc" target="_blank">[YouTube]</a>.
                </p>
              <li>
              <li>
                <p class="text-small-margin">
                  <b> NeurIPS 2023 Presentation</b>, Unified 3D Segmenter as Prototypical Classifiers. New Orleans, USA.
                </p>
              <li>
              <li>
                <p class="text-small-margin">
                  <b> ICLR 2024 Presentation</b>, Facing the Elephant in the Room: Visual Prompt Tuning or Full Finetuning?
                </p>
              <li>
              <li>
                <p class="text-small-margin">
                  <b> ICLR 2024 Presentation</b>, Image Translation as Diffusion Visual Programmers.
                </p>
              <li>
              <li>
                <p class="text-small-margin">
                  NSF REU SITE: AI-empowered Cybersecurity Seminar Series (07/12/2024): Prompts as Sustainable Learners.
                </p>
              <li>
              <li>
                <p class="text-small-margin">
                  <b> EMNLP 2024 Presentation</b>, M^2PT: Multimodal Prompt Tuning for Zero-shot Instruction Learning (11/14/2024).
                </p>
              <li>
              <li>
                <p class="text-small-margin">
                  Naval Research Laboratory Machine Learning Seminar (12/04/2024): Parameter Efficient Finetuning in Large-scale AI Models.
                </p>
              <li>
              <li>
                <p class="text-small-margin">
                  NSF LEVEL UP AI Roundtable (04/25/2025): Principles and Resources for AI Education for All.
                </p>
              <li>
              <li>
                <p class="text-small-margin">
                  The Computing Community Consortium (CCC) Computing Futures Symposium (05/15/2025-05/16/2025): Re-Imagining Multimodal Instruction Tuning from a Parameter-efficient Representation Perspective.
                </p>
              <li>
              <li>
                <p class="text-small-margin">
                  NSF REU SITE: AI-empowered Cybersecurity Seminar Series (06/30/2025): Efficient Fine-Tuning for X.
                </p>
              <li>
          </ol>
          <h2 id="Teaching">Teaching</h2>
          <hr>
          <ol class="publication A-list">
          <li>
            <p class="text-small-margin">
              <b> UMKC COMP-SCI 5567-0001: Deep Learning</b> Spring, 2025.
            </p>
          <li>
          </ol> 
          <!--End Projects-->
        </div>
      </div>
    </div>
  </div>
</body>

</html>
